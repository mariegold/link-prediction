{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Embeddings\n",
    "\n",
    "In this notebook, we train 2 types of embeddings (node2vec and Laplacian eigenmaps) and fit a logistic regression for the `ogbl-ddi` link prediction task. \n",
    "\n",
    "##### **It is recommended to download the pre-computed random walks [here](https://drive.google.com/file/d/1H8o26Yztwc3IRNDo6Ir8-qeZaG49vIqC/view?usp=sharing) to save time.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch_geometric\n",
    "import networkx as nx\n",
    "import random\n",
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "import pickle\n",
    "\n",
    "from karateclub import LaplacianEigenmaps\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from torch_geometric.utils import negative_sampling\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "load_walks = os.path.exists('walks.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ogb.linkproppred import PygLinkPropPredDataset, Evaluator\n",
    "dataset = PygLinkPropPredDataset(name = 'ogbl-ddi')\n",
    "evaluator = Evaluator(name='ogbl-ddi')\n",
    "\n",
    "split_edge = dataset.get_edge_split()\n",
    "train_edge, valid_edge, test_edge = split_edge[\"train\"], split_edge[\"valid\"], split_edge[\"test\"]\n",
    "graph = dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of edges in the training set:  1067911\n",
      "Number of edges in the validation set: \n",
      "\t positive:  133489\n",
      "\t negative:  101882\n",
      "Number of edges in the test set: \n",
      "\t positive:  133489\n",
      "\t negative:  95599\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of edges in the training set: \", train_edge['edge'].shape[0])\n",
    "print(\"Number of edges in the validation set: \")\n",
    "print(\"\\t positive: \", valid_edge['edge'].shape[0])\n",
    "print(\"\\t negative: \", valid_edge['edge_neg'].shape[0])\n",
    "print(\"Number of edges in the test set: \")\n",
    "print(\"\\t positive: \", test_edge['edge'].shape[0])\n",
    "print(\"\\t negative: \", test_edge['edge_neg'].shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "eval metric                                                        hits@20\n",
       "task type                                                  link prediction\n",
       "download_name                                                          ddi\n",
       "version                                                                  1\n",
       "url                      http://snap.stanford.edu/ogb/data/linkproppred...\n",
       "add_inverse_edge                                                      True\n",
       "has_node_attr                                                        False\n",
       "has_edge_attr                                                        False\n",
       "split                                                               target\n",
       "additional node files                                                 None\n",
       "additional edge files                                                 None\n",
       "is hetero                                                            False\n",
       "binary                                                               False\n",
       "Name: ogbl-ddi, dtype: object"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.meta_info"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Node2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-06-09 13:06:03.197948: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "from gensim.models import Word2Vec\n",
    "from stellargraph.data import BiasedRandomWalk\n",
    "from stellargraph import StellarGraph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Node2Vec(G, dimensions, walk_length, num_walks, p=1.0, q=1.0, seed=0, load_walks=True):\n",
    "  '''G: NetworkX graph object\n",
    "      dimensions: embedding dimensions\n",
    "      walk_length: maximum length of each random walk\n",
    "      num_walks: total number of random walks per root node\n",
    "      q (float, > 0): In-out parameter; q controls the likelihood of moving away from source node.\n",
    "        If q > 1, the RW is biased towards closeby points (BFS), better for capturing structural nodes\n",
    "        If q < 1, the RW is biased towards faraway points (DFS), better for capturing communities\n",
    "      p (float, > 0): Return parameter; p controls the likelihood of immediately revisiting a node in the walk.\n",
    "        If p > max(q, 1)) makes it less likely to revisit a node in the next two steps (unless there is no other node available). \n",
    "        If p < min(q, 1)), it keeps the walk “local” (i.e. close to the starting node)\n",
    "  '''\n",
    "  if load_walks:\n",
    "      print(\"Loading walks...\")\n",
    "      with open('walks.pkl', 'rb') as f:\n",
    "        walks = pickle.load(f)\n",
    "  else:\n",
    "    rw = BiasedRandomWalk(G, seed=seed)\n",
    "\n",
    "    walks = rw.run(nodes=list(G.nodes()), length=walk_length, n=num_walks, p=p, q=q, seed=seed)\n",
    "\n",
    "    with open('walks.pkl', 'wb') as f:\n",
    "        pickle.dump(walks, f)\n",
    "\n",
    "    print(\"Done walking...\")\n",
    "\n",
    "  str_walks = [[str(n) for n in walk] for walk in walks]\n",
    "  model = Word2Vec(\n",
    "      str_walks, vector_size=dimensions, min_count=0, sg=1, workers=-1, seed=seed, epochs=100\n",
    "  )\n",
    "\n",
    "  return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helpers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the features: concatenation of node embeddings\n",
    "def embedded_edgelist(model, edgelist, type='node2vec'):\n",
    "    \"\"\" Returns the features which are the Hadamard product of the node embeddings.\n",
    "\n",
    "    Params:\n",
    "    model: Embeddings (gensim or numpy).\n",
    "    edgelist (np.array): Node pairs.\n",
    "    type (str): If 'node2vec', embeddings should be from `gensim`.\n",
    "    \n",
    "    Returns:\n",
    "    np.array: `Nxd` feature vector.\n",
    "    \"\"\"\n",
    "    edgelist = edgelist.numpy()\n",
    "    if type == 'node2vec':\n",
    "        return np.stack([(model.wv.get_vector(x[0])*model.wv.get_vector(x[1])).transpose() for x in edgelist])\n",
    "    else:\n",
    "        return np.stack([(model[x[0]]*model[x[1]]).transpose() for x in edgelist])\n",
    "    \n",
    "    \n",
    "def get_train_data(graph, train_edges, d=32, type='node2vec', load_walks=True):\n",
    "    \"\"\"Prepares the training data.\n",
    "\n",
    "    d (int): Embedding dimension.\n",
    "    type (str): Which embedding model to fit.\n",
    "    load_walks (bool): Use precomputed random walks from `walks.pkl`.\n",
    "    \n",
    "    Returns:\n",
    "    emb (numpy or gensim): Embeddings\n",
    "    feats (np.array): Training features.\n",
    "    labels (np.array): Training labels.\n",
    "    \"\"\"\n",
    "    g = torch_geometric.utils.to_networkx(graph, to_undirected=True)\n",
    "    \n",
    "    # Train the embedding\n",
    "    if type == 'node2vec':\n",
    "        emb = Node2Vec(StellarGraph.from_networkx(g), dimensions=d, walk_length=5, \n",
    "            num_walks=1000, p=1.0, q=0.5, load_walks=load_walks) \n",
    "    else:\n",
    "        model = LaplacianEigenmaps(dimensions=d)\n",
    "        model.fit(g) \n",
    "        emb = model.get_embedding()\n",
    "\n",
    "    # Extract positive and negative samples\n",
    "    num_edges = len(train_edges)\n",
    "    neg_edges = negative_sampling(graph.edge_index, num_nodes=len(g.nodes),\n",
    "                                    num_neg_samples=num_edges, method='dense')\n",
    "\n",
    "    feats_pos = embedded_edgelist(emb, train_edges, type)\n",
    "    feats_neg = embedded_edgelist(emb, neg_edges, type)\n",
    "    feats = np.concatenate((feats_pos, feats_neg))\n",
    "    labels = np.concatenate((np.repeat(1, feats_pos.shape[0]), np.repeat(0, feats_neg.shape[0])))\n",
    "    return emb, feats, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval(clf, emb, type='node2vec', split_edge=split_edge, evaluator=evaluator):\n",
    "    \"\"\" Evaluates the model.\n",
    "\n",
    "    clf: Fitted regression model (scikit)\n",
    "    emb: Trained embeddings (numpy or gensim)\n",
    "    type: Which model we're evaluating. Defaults to 'node2vec'.\n",
    "        If 'node2vec', embeddings should be  from gensim.\n",
    "    split_edge: Train/validation/test split of the data.  \n",
    "    evaluator: Evaluator for the `ogbl-ddi` task.  \n",
    "    \"\"\"\n",
    "    feats_pos_val = embedded_edgelist(emb, split_edge['valid']['edge'], type)\n",
    "    feats_neg_val = embedded_edgelist(emb, split_edge['valid']['edge_neg'], type)\n",
    "    feats_pos_test = embedded_edgelist(emb, split_edge['test']['edge'], type)\n",
    "    feats_neg_test = embedded_edgelist(emb, split_edge['test']['edge_neg'], type)\n",
    "    feats_pos_train = embedded_edgelist(emb, split_edge['train']['edge'], type)\n",
    "\n",
    "    preds_pos_val = clf.predict_proba(feats_pos_val).max(axis=1)\n",
    "    preds_neg_val = clf.predict_proba(feats_neg_val).max(axis=1)\n",
    "    preds_pos_test = clf.predict_proba(feats_pos_test).max(axis=1)\n",
    "    preds_neg_test = clf.predict_proba(feats_neg_test).max(axis=1)\n",
    "    preds_pos_train = clf.predict_proba(feats_pos_train).max(axis=1)\n",
    "\n",
    "    for K in [10, 20, 30]:\n",
    "        evaluator.K = K\n",
    "        valid_hits = evaluator.eval({\n",
    "                    'y_pred_pos': preds_pos_val,\n",
    "                    'y_pred_neg': preds_neg_val,\n",
    "                })[f'hits@{K}']\n",
    "        print(\"val hits@{}: {:.5f}\".format(K, valid_hits))\n",
    "        test_hits = evaluator.eval({\n",
    "                    'y_pred_pos': preds_pos_test,\n",
    "                    'y_pred_neg': preds_neg_test,\n",
    "                })[f'hits@{K}']\n",
    "        print(\"test hits@{}: {:.5f}\".format(K, test_hits))\n",
    "        train_hits = evaluator.eval({\n",
    "                'y_pred_pos': preds_pos_train,\n",
    "                'y_pred_neg': preds_neg_val,\n",
    "            })[f'hits@{K}']\n",
    "        print(\"train hits@{}: {:.5f}\".format(K, train_hits))\n",
    "        print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Node2vec, $d=32$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading walks...\n",
      "val hits@10: 0.00010\n",
      "test hits@10: 0.00016\n",
      "train hits@10: 0.00010\n",
      "\n",
      "\n",
      "val hits@20: 0.00022\n",
      "test hits@20: 0.00025\n",
      "train hits@20: 0.00019\n",
      "\n",
      "\n",
      "val hits@30: 0.00034\n",
      "test hits@30: 0.00033\n",
      "train hits@30: 0.00031\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "emb, feats_train, labels_train = get_train_data(dataset[0], split_edge['train']['edge'], load_walks=load_walks)\n",
    "clf = make_pipeline(StandardScaler(), LogisticRegression()).fit(feats_train, labels_train)\n",
    "eval(clf, emb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Laplacian eigenmaps\n",
    "##### $d=32$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val hits@10: 0.01781\n",
      "test hits@10: 0.04971\n",
      "train hits@10: 0.02011\n",
      "\n",
      "\n",
      "val hits@20: 0.02569\n",
      "test hits@20: 0.05865\n",
      "train hits@20: 0.02913\n",
      "\n",
      "\n",
      "val hits@30: 0.03481\n",
      "test hits@30: 0.07049\n",
      "train hits@30: 0.03955\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "emb, feats_train, labels_train = get_train_data(dataset[0], split_edge['train']['edge'], type='eigenmaps')\n",
    "clf = make_pipeline(StandardScaler(), LogisticRegression()).fit(feats_train, labels_train)\n",
    "eval(clf, emb, type=\"eigenmaps\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### $d=64$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val hits@10: 0.00000\n",
      "test hits@10: 0.00000\n",
      "train hits@10: 0.00000\n",
      "\n",
      "\n",
      "val hits@20: 0.01463\n",
      "test hits@20: 0.03833\n",
      "train hits@20: 0.01660\n",
      "\n",
      "\n",
      "val hits@30: 0.02428\n",
      "test hits@30: 0.05552\n",
      "train hits@30: 0.02758\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "emb, feats_train, labels_train = get_train_data(dataset[0], split_edge['train']['edge'], d=64, type='eigenmaps')\n",
    "clf = make_pipeline(StandardScaler(), LogisticRegression()).fit(feats_train, labels_train)\n",
    "eval(clf, emb, type=\"eigenmaps\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### $d=16$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val hits@10: 0.02084\n",
      "test hits@10: 0.04959\n",
      "train hits@10: 0.02345\n",
      "\n",
      "\n",
      "val hits@20: 0.02594\n",
      "test hits@20: 0.05868\n",
      "train hits@20: 0.02936\n",
      "\n",
      "\n",
      "val hits@30: 0.03549\n",
      "test hits@30: 0.07048\n",
      "train hits@30: 0.04047\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "emb, feats_train, labels_train = get_train_data(dataset[0], split_edge['train']['edge'], d=16, type='eigenmaps')\n",
    "clf = make_pipeline(StandardScaler(), LogisticRegression()).fit(feats_train, labels_train)\n",
    "eval(clf, emb, type=\"eigenmaps\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### $d=8$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val hits@10: 0.02104\n",
      "test hits@10: 0.04643\n",
      "train hits@10: 0.02381\n",
      "\n",
      "\n",
      "val hits@20: 0.02534\n",
      "test hits@20: 0.05521\n",
      "train hits@20: 0.02872\n",
      "\n",
      "\n",
      "val hits@30: 0.03291\n",
      "test hits@30: 0.06858\n",
      "train hits@30: 0.03757\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "emb, feats_train, labels_train = get_train_data(dataset[0], split_edge['train']['edge'], d=8, type='eigenmaps')\n",
    "clf = make_pipeline(StandardScaler(), LogisticRegression()).fit(feats_train, labels_train)\n",
    "eval(clf, emb, type=\"eigenmaps\")"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "672a148edff965e1f25bd0b0da530a9c3c1c3d30d9b12282e6549c14cf941e6c"
  },
  "kernelspec": {
   "display_name": "Python 3.8.11 ('networks')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
